{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "\n",
    "\n",
    "## Homework 2 - Deep Neural Networks, Classifiers & Features\n",
    "---\n",
    "\n",
    "### <a style='color:red'> Due Date: 20.05.2021 </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "#### READ THIS CAREFULLY\n",
    "* Submission only in **pairs**.\n",
    "* **No handwritten submissions**.\n",
    "* You can choose your working environment:\n",
    "    * You can work in a `Jupyter Notebook`, locally with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or online on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * **Important**: Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    * You can work in a Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both also allow opening/editing Jupyter Notebooks.\n",
    "* You should submit two **separated** files:\n",
    "    * A compressed `.zip` file, with the name: `ee046746_hw2_id1_id2.zip` which contains:\n",
    "        * A folder named `code` with all the code files inside (`.py` or `.ipynb` ONLY!), and all the files required for the code to run (your own images/videos).\n",
    "            * **The code should run both on CPU and GPU without manual modifications**, require no special preparation and run on every computer.\n",
    "    * A report file (visualizations, discussing the results and answering the questions) in a `.pdf` format, with the name `ee046746_hw2_id1_id2.pdf`.\n",
    "    * **DON'T** submit the SVHN dataset in your submission, we have a local copy of it.\n",
    "    * **DON'T** submit the trained networks, but make sure to document every result you get.\n",
    "        * Be precise, we expect on point answers.\n",
    "        * No other file-types (`.docx`, `.html`, ...) will be accepted.\n",
    "* Submission on the course website (Moodle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/python.png\" style=\"height:50px;display:inline\"> Python Libraries\n",
    "---\n",
    "\n",
    "* `numpy`\n",
    "* `matplotlib`\n",
    "* `pytorch` (and `torchvision`)\n",
    "* `opencv` (or `scikit-image`)\n",
    "* `scikit-learn`\n",
    "* Anything else you need (`PIL`, `os`, `pandas`, `csv`, `json`,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Tasks\n",
    "---\n",
    "* In all tasks, you should document your process and results in a report file (which will be saved as `.pdf`). \n",
    "* You can reference your code in the report file, but no need for actual code in this file, the code is submitted in a seprate folder as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 - Design and Build a CNN Classifier\n",
    "---\n",
    "In this part you are going to design a deep convolutional neural network to classify house number digits from the **The Street View House Numbers (SVHN)** Dataset. \n",
    "\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
    "\n",
    "* 10 classes, 1 for each digit. Digit '0' has label 0, '1' has label 1,...\n",
    "* 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data.\n",
    "\n",
    "<img src=\"http://ufldl.stanford.edu/housenumbers/32x32eg.png\" style=\"height:250px\">\n",
    "\n",
    "1. Load the SVHN dataset with PyTorch using `torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=True)` (<a href=\"https://pytorch.org/docs/stable/torchvision/datasets.html#svhn\"> read more here</a>). Display 5 images from the train set.\n",
    "2. Use the CNN from tutorials 3-4 (`CifarCnn()`) and train it on the SVHN dataset (**keep the architecture the same**) for the same number of **epochs** with the same learning rate (try to keep the batch size the same, but if you get memory errors, you can reduce it). What is the accuracy on the test set? What classes are most confusing for this model?\n",
    "    * Your test accuracy from this section will be your **baseline** accuracy for sections 3-4.\n",
    "3. Design a Convolutional Neural Network (CNN) to classify digits from the images. You can modify the network from section 2, but **you must get a better result than your baseline accuracy from section 2**.\n",
    "    * Describe the chosen architecture, how many layers? What activations did you choose? What are the filter sizes? Did you use fully-connected layers (if you did, explain their sizes)?\n",
    "    * What is the input dimension? What is the output dimension?\n",
    "    * Calculate the number of parameters (weights) in the network.\n",
    "4. Train the classifier (preferably on a **GPU - use Colab for this part** if you don't have a GPU).\n",
    "    * Describe the the hyper-parameters of the model (batch size, epochs, learning rate....). How did you tune your model? Did you use a validation set to tune the model? (<a href=\"https://gist.github.com/MattKleinsmith/5226a94bad5dd12ed0b871aed98cb123\">Separating to train/validation/test in PyTorch</a>)\n",
    "    * What is the final accuracy on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - Analyzing a Pre-trained CNN\n",
    "---\n",
    "In this part you are going to analyze a (large) pre-trained model. Pre-trained models are quite popular these days, as big companies can train really large models on large datasets (something that personal users can't do as they lack the sufficient hardware). These pre-trained models can be used to fine-tune on other/small datasets or used as components in other tasks (like using a pre-trained classifier for object detection).\n",
    "\n",
    "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`. \n",
    "\n",
    "You can use the following transform to normalize:\n",
    "\n",
    "`normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`\n",
    "\n",
    "<a href=\"https://pytorch.org/vision/stable/models.html\">Read more here</a>\n",
    "\n",
    "1. Load a pre-trained VGG16 with PyTorch using `torchvision.models.vgg16(pretrained=True, progress=True, **kwargs)` (<a href=\"https://pytorch.org/vision/stable/models.html#classification\">read more here</a>). Don't forget to use the model in evaluation mode (`model.eval()`). \n",
    "2. Load the images in the `./birds` folder and display them.\n",
    "3. Pre-process the images to fit VGG16's architecture. What steps did you take?\n",
    "4. Feed the images (forward pass) to the model. What are the outputs?\n",
    "5. Find an image of a bird/cat/dog on the internet, display it and feed it to network. What are the outputs?\n",
    "6. Apply the following 3 transformations to create 3 *new* images from the image from step 5, and display them (`opencv` has functions for all):\n",
    "    * One **geometric transformation** (rotation, scaling, translation, warping...).\n",
    "    * One **color transformation** (thresholding, different color space, hue, saturation, brightness, contrast...).\n",
    "    * One **filter** (any filter you want).\n",
    "7. Feed the transformed images to network, what is the output? is it different than section 5?\n",
    "8. For the first 3 filters in the *first layer* of VGG16, plot the filters, and then plot their response (their output) for the image from section 5 and the 3 images from section 6 (total of 4 input images). Explain what do you see.\n",
    "    * Consult `ee046746_appndx_visualizing_cnn_filters.ipynb` to refresh your memory.\n",
    "9. For each image in the `./dogs` and `./cats` folders, extract and save their feature vectors (create a numpy array or a torch tensor that contains the features for all samples) from a fully-connected layer (such as `FC7`) of the VGG16 model. Which layer did you pick? What is the size of the feature space?\n",
    "    * You need to write a function that does the feed forward manually until the desired layer. See the example in `ee046746_appndx_visualizing_cnn_filters.ipynb`. \n",
    "10. Build a Support Vector Machine (SVM) classifier (hint: `sklearn.svm.LinearSVC`) to classify cats and dogs based on the features you extracted. Use the 20 images as train set, and choose 4 images (2 dogs, 2 cats) from the internet as test sets. You can choose a different classifer than SVM from the `scikit-learn` library, no need to explain how it works (but report the name of the algorithm you used). What are the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
